{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gen_trainval_list.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'maize_counting_dataset'\n",
    "image_folder = 'images'\n",
    "label_folder = 'labels'\n",
    "train = 'train'\n",
    "val = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(root, train)\n",
    "with open('train.txt', 'w') as f:\n",
    "    for image_path in glob.glob(os.path.join(train_path, image_folder, '*.JPG')):\n",
    "        im_path = image_path.replace(root, '')\n",
    "        gt_path = im_path.replace(image_folder, label_folder).replace('.JPG', '.xml')\n",
    "        f.write(im_path+'\\t'+gt_path+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = os.path.join(root, val)\n",
    "with open('val.txt', 'w') as f:\n",
    "    for image_path in glob.glob(os.path.join(val_path, image_folder, '*.JPG')):\n",
    "        im_path = image_path.replace(root, '')\n",
    "        gt_path = im_path.replace(image_folder, label_folder).replace('.JPG', '.xml')\n",
    "        f.write(im_path+'\\t'+gt_path+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hldataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd \n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import h5py\n",
    "import scipy.io as sio\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from skimage import util\n",
    "from skimage.measure import label\n",
    "from skimage.measure import regionprops\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(x):\n",
    "    img_arr = np.array(Image.open(x))\n",
    "    if len(img_arr.shape) == 2:  # grayscale\n",
    "        img_arr = np.tile(img_arr, [3, 1, 1]).transpose(1, 2, 0)\n",
    "    return img_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCrop(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        image, target, gtcount = sample['image'], sample['target'], sample['gtcount']\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        if isinstance(self.output_size, tuple):\n",
    "            new_h = min(self.output_size[0], h)\n",
    "            new_w = min(self.output_size[1], w)\n",
    "            assert (new_h, new_w) == self.output_size\n",
    "        else:\n",
    "            crop_size = min(self.output_size, h, w)\n",
    "            assert crop_size == self.output_size\n",
    "            new_h = new_w = crop_size\n",
    "        if gtcount > 0:\n",
    "            mask = target > 0\n",
    "            ch, cw = int(np.ceil(new_h / 2)), int(np.ceil(new_w / 2))\n",
    "            mask_center = np.zeros((h, w), dtype=np.uint8)\n",
    "            mask_center[ch:h-ch+1, cw:w-cw+1] = 1\n",
    "            mask = (mask & mask_center)\n",
    "            idh, idw = np.where(mask == 1)\n",
    "            if len(idh) != 0:\n",
    "                ids = random.choice(range(len(idh)))\n",
    "                hc, wc = idh[ids], idw[ids]\n",
    "                top, left = hc-ch, wc-cw\n",
    "            else:\n",
    "                top = np.random.randint(0, h-new_h+1)\n",
    "                left = np.random.randint(0, w-new_w+1)\n",
    "        else:\n",
    "            top = np.random.randint(0, h-new_h+1)\n",
    "            left = np.random.randint(0, w-new_w+1)\n",
    "\n",
    "        image = image[top:top+new_h, left:left+new_w, :]\n",
    "        target = target[top:top+new_h, left:left+new_w]\n",
    "\n",
    "        return {'image': image, 'target': target, 'gtcount': gtcount}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomFlip(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, target, gtcount = sample['image'], sample['target'], sample['gtcount']\n",
    "        do_mirror = np.random.randint(2)\n",
    "        if do_mirror:\n",
    "            image = cv2.flip(image, 1)\n",
    "            target = cv2.flip(target, 1)\n",
    "        return {'image': image, 'target': target, 'gtcount': gtcount}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "\n",
    "    def __init__(self, scale, mean, std):\n",
    "        self.scale = scale\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, target, gtcount = sample['image'], sample['target'], sample['gtcount']\n",
    "        image, target = image.astype('float32'), target.astype('float32')\n",
    "\n",
    "        # pixel normalization\n",
    "        image = (self.scale * image - self.mean) / self.std\n",
    "\n",
    "        image, target = image.astype('float32'), target.astype('float32')\n",
    "\n",
    "        return {'image': image, 'target': target, 'gtcount': gtcount}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroPadding(object):\n",
    "    def __init__(self, psize=32):\n",
    "        self.psize = psize\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        psize =  self.psize\n",
    "\n",
    "        image, target, gtcount = sample['image'], sample['target'], sample['gtcount']\n",
    "        h,w = image.size()[-2:]\n",
    "        ph,pw = (psize-h%psize),(psize-w%psize)\n",
    "        # print(ph,pw)\n",
    "\n",
    "        (pl, pr) = (pw//2, pw-pw//2) if pw != psize else (0, 0)\n",
    "        (pt, pb) = (ph//2, ph-ph//2) if ph != psize else (0, 0)\n",
    "        if (ph!=psize) or (pw!=psize):\n",
    "            tmp_pad = [pl, pr, pt, pb]\n",
    "            # print(tmp_pad)\n",
    "            image = F.pad(image,tmp_pad)\n",
    "            target = F.pad(target,tmp_pad)\n",
    "\n",
    "        return {'image': image, 'target': target, 'gtcount': gtcount}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # swap color axis\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image, target, gtcount = sample['image'], sample['target'], sample['gtcount']\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        target = np.expand_dims(target, axis=2)\n",
    "        target = target.transpose((2, 0, 1))\n",
    "        image, target = torch.from_numpy(image), torch.from_numpy(target)\n",
    "        return {'image': image, 'target': target, 'gtcount': gtcount}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaizeTasselDataset(Dataset):\n",
    "    def __init__(self, data_dir, data_list, ratio, train=True, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.data_list = [name.split('\\t') for name in open(data_list).read().splitlines()]\n",
    "        self.ratio = ratio\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.image_list = []\n",
    "        \n",
    "        # store images and generate ground truths\n",
    "        self.images = {}\n",
    "        self.targets = {}\n",
    "        self.gtcounts = {}\n",
    "        self.dotimages = {}\n",
    "\n",
    "    def bbs2points(self, bbs):    \n",
    "        points = []\n",
    "        for bb in bbs:\n",
    "            x1, y1, w, h = [float(b) for b in bb]\n",
    "            x2, y2 = x1+w-1, y1+h-1\n",
    "            x, y = np.round((x1+x2)/2).astype(np.int32), np.round((y1+y2)/2).astype(np.int32)\n",
    "            points.append([x, y])\n",
    "        return points\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.data_list[idx]\n",
    "        self.image_list.append(file_name[0])\n",
    "        if file_name[0] not in self.images:\n",
    "            image = read_image(self.data_dir+file_name[0])\n",
    "            annotation = sio.loadmat(self.data_dir+file_name[1])\n",
    "            h, w = image.shape[:2]\n",
    "            nh = int(np.ceil(h * self.ratio))\n",
    "            nw = int(np.ceil(w * self.ratio))\n",
    "            image = cv2.resize(image, (nw, nh), interpolation = cv2.INTER_CUBIC)\n",
    "            target = np.zeros((nh, nw), dtype=np.float32)\n",
    "            dotimage = image.copy()\n",
    "            if annotation['annotation'][0][0][1] is not None:\n",
    "                bbs = annotation['annotation'][0][0][1]\n",
    "                gtcount = bbs.shape[0]\n",
    "                pts = self.bbs2points(bbs)\n",
    "                for pt in pts:\n",
    "                    pt[0], pt[1] = int(pt[0] * self.ratio), int(pt[1] * self.ratio)\n",
    "                    target[pt[1], pt[0]] = 1\n",
    "                    cv2.circle(dotimage, (pt[0], pt[1]), int(24 * self.ratio) , (255, 0, 0), -1)\n",
    "            else:\n",
    "                gtcount = 0\n",
    "            target = gaussian_filter(target, 80 * self.ratio)\n",
    "\n",
    "            # plt.imshow(target, cmap=cm.jet)\n",
    "            # plt.show()\n",
    "            # print(target.sum())\n",
    "\n",
    "            self.images.update({file_name[0]:image})\n",
    "            self.targets.update({file_name[0]:target})\n",
    "            self.gtcounts.update({file_name[0]:gtcount})\n",
    "            self.dotimages.update({file_name[0]:dotimage})\n",
    "\n",
    "        \n",
    "        sample = {\n",
    "            'image': self.images[file_name[0]], \n",
    "            'target': self.targets[file_name[0]], \n",
    "            'gtcount': self.gtcounts[file_name[0]]\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MaizeTasselDataset(\n",
    "        data_dir='maize_counting_dataset', \n",
    "        data_list='maize_counting_dataset/train.txt',\n",
    "        ratio=0.167, \n",
    "        train=True, \n",
    "        transform=transforms.Compose([\n",
    "            ToTensor()]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "torch.Size([1, 3, 278770])\n",
      "0\n",
      "torch.Size([1, 3, 278770])\n",
      "1\n",
      "torch.Size([1, 3, 278770])\n",
      "2\n",
      "torch.Size([1, 3, 278770])\n",
      "3\n",
      "torch.Size([1, 3, 278770])\n",
      "4\n",
      "torch.Size([1, 3, 222530])\n",
      "5\n",
      "torch.Size([1, 3, 278770])\n",
      "6\n",
      "torch.Size([1, 3, 222530])\n",
      "7\n",
      "torch.Size([1, 3, 278770])\n",
      "8\n",
      "torch.Size([1, 3, 278770])\n",
      "9\n",
      "torch.Size([1, 3, 278770])\n",
      "10\n",
      "torch.Size([1, 3, 278770])\n",
      "11\n",
      "torch.Size([1, 3, 278770])\n",
      "12\n",
      "torch.Size([1, 3, 278770])\n",
      "13\n",
      "torch.Size([1, 3, 278770])\n",
      "14\n",
      "torch.Size([1, 3, 278770])\n",
      "15\n",
      "torch.Size([1, 3, 278770])\n",
      "16\n",
      "torch.Size([1, 3, 278770])\n",
      "17\n",
      "torch.Size([1, 3, 278770])\n",
      "18\n",
      "torch.Size([1, 3, 278770])\n",
      "19\n",
      "torch.Size([1, 3, 278770])\n",
      "20\n",
      "torch.Size([1, 3, 278770])\n",
      "21\n",
      "torch.Size([1, 3, 278770])\n",
      "22\n",
      "torch.Size([1, 3, 278770])\n",
      "23\n",
      "torch.Size([1, 3, 278770])\n",
      "24\n",
      "torch.Size([1, 3, 278770])\n",
      "25\n",
      "torch.Size([1, 3, 278770])\n",
      "26\n",
      "torch.Size([1, 3, 278770])\n",
      "27\n",
      "torch.Size([1, 3, 278770])\n",
      "28\n",
      "torch.Size([1, 3, 278770])\n",
      "29\n",
      "torch.Size([1, 3, 278770])\n",
      "30\n",
      "torch.Size([1, 3, 278770])\n",
      "31\n",
      "torch.Size([1, 3, 278770])\n",
      "32\n",
      "torch.Size([1, 3, 278770])\n",
      "33\n",
      "torch.Size([1, 3, 278770])\n",
      "34\n",
      "torch.Size([1, 3, 278770])\n",
      "35\n",
      "torch.Size([1, 3, 278770])\n",
      "36\n",
      "torch.Size([1, 3, 278770])\n",
      "37\n",
      "torch.Size([1, 3, 278770])\n",
      "38\n",
      "torch.Size([1, 3, 278770])\n",
      "39\n",
      "torch.Size([1, 3, 278770])\n",
      "40\n",
      "torch.Size([1, 3, 278770])\n",
      "41\n",
      "torch.Size([1, 3, 278770])\n",
      "42\n",
      "torch.Size([1, 3, 278770])\n",
      "43\n",
      "torch.Size([1, 3, 278770])\n",
      "44\n",
      "torch.Size([1, 3, 278770])\n",
      "45\n",
      "torch.Size([1, 3, 278770])\n",
      "46\n",
      "torch.Size([1, 3, 222530])\n",
      "47\n",
      "torch.Size([1, 3, 278770])\n",
      "48\n",
      "torch.Size([1, 3, 278770])\n",
      "49\n",
      "torch.Size([1, 3, 278770])\n",
      "50\n",
      "torch.Size([1, 3, 278770])\n",
      "51\n",
      "torch.Size([1, 3, 278770])\n",
      "52\n",
      "torch.Size([1, 3, 278770])\n",
      "53\n",
      "torch.Size([1, 3, 278770])\n",
      "54\n",
      "torch.Size([1, 3, 278770])\n",
      "55\n",
      "torch.Size([1, 3, 278770])\n",
      "56\n",
      "torch.Size([1, 3, 278770])\n",
      "57\n",
      "torch.Size([1, 3, 278770])\n",
      "58\n",
      "torch.Size([1, 3, 278770])\n",
      "59\n",
      "torch.Size([1, 3, 278770])\n",
      "60\n",
      "torch.Size([1, 3, 222530])\n",
      "61\n",
      "torch.Size([1, 3, 278770])\n",
      "62\n",
      "torch.Size([1, 3, 278770])\n",
      "63\n",
      "torch.Size([1, 3, 278770])\n",
      "64\n",
      "torch.Size([1, 3, 278770])\n",
      "65\n",
      "torch.Size([1, 3, 278770])\n",
      "66\n",
      "torch.Size([1, 3, 222530])\n",
      "67\n",
      "torch.Size([1, 3, 278770])\n",
      "68\n",
      "torch.Size([1, 3, 278770])\n",
      "69\n",
      "torch.Size([1, 3, 278770])\n",
      "70\n",
      "torch.Size([1, 3, 278770])\n",
      "71\n",
      "torch.Size([1, 3, 278770])\n",
      "72\n",
      "torch.Size([1, 3, 278770])\n",
      "73\n",
      "torch.Size([1, 3, 278770])\n",
      "74\n",
      "torch.Size([1, 3, 222530])\n",
      "75\n",
      "torch.Size([1, 3, 278770])\n",
      "76\n",
      "torch.Size([1, 3, 278770])\n",
      "77\n",
      "torch.Size([1, 3, 278770])\n",
      "78\n",
      "torch.Size([1, 3, 278770])\n",
      "79\n",
      "torch.Size([1, 3, 278770])\n",
      "80\n",
      "torch.Size([1, 3, 278770])\n",
      "81\n",
      "torch.Size([1, 3, 278770])\n",
      "82\n",
      "torch.Size([1, 3, 278770])\n",
      "83\n",
      "torch.Size([1, 3, 278770])\n",
      "84\n",
      "torch.Size([1, 3, 278770])\n",
      "85\n",
      "torch.Size([1, 3, 278770])\n",
      "86\n",
      "torch.Size([1, 3, 278770])\n",
      "87\n",
      "torch.Size([1, 3, 278770])\n",
      "88\n",
      "torch.Size([1, 3, 278770])\n",
      "89\n",
      "torch.Size([1, 3, 278770])\n",
      "90\n",
      "torch.Size([1, 3, 278770])\n",
      "91\n",
      "torch.Size([1, 3, 222530])\n",
      "92\n",
      "torch.Size([1, 3, 278770])\n",
      "93\n",
      "torch.Size([1, 3, 278770])\n",
      "94\n",
      "torch.Size([1, 3, 278770])\n",
      "95\n",
      "torch.Size([1, 3, 222530])\n",
      "96\n",
      "torch.Size([1, 3, 278770])\n",
      "97\n",
      "torch.Size([1, 3, 278770])\n",
      "98\n",
      "torch.Size([1, 3, 278770])\n",
      "99\n",
      "torch.Size([1, 3, 278770])\n",
      "100\n",
      "torch.Size([1, 3, 278770])\n",
      "101\n",
      "torch.Size([1, 3, 278770])\n",
      "102\n",
      "torch.Size([1, 3, 278770])\n",
      "103\n",
      "torch.Size([1, 3, 278770])\n",
      "104\n",
      "torch.Size([1, 3, 278770])\n",
      "105\n",
      "torch.Size([1, 3, 278770])\n",
      "106\n",
      "torch.Size([1, 3, 278770])\n",
      "107\n",
      "torch.Size([1, 3, 278770])\n",
      "108\n",
      "torch.Size([1, 3, 278770])\n",
      "109\n",
      "torch.Size([1, 3, 278770])\n",
      "110\n",
      "torch.Size([1, 3, 278770])\n",
      "111\n",
      "torch.Size([1, 3, 278770])\n",
      "112\n",
      "torch.Size([1, 3, 222530])\n",
      "113\n",
      "torch.Size([1, 3, 278770])\n",
      "114\n",
      "torch.Size([1, 3, 278770])\n",
      "115\n",
      "torch.Size([1, 3, 278770])\n",
      "116\n",
      "torch.Size([1, 3, 278770])\n",
      "117\n",
      "torch.Size([1, 3, 278770])\n",
      "118\n",
      "torch.Size([1, 3, 278770])\n",
      "119\n",
      "torch.Size([1, 3, 278770])\n",
      "120\n",
      "torch.Size([1, 3, 278770])\n",
      "121\n",
      "torch.Size([1, 3, 278770])\n",
      "122\n",
      "torch.Size([1, 3, 278770])\n",
      "123\n",
      "torch.Size([1, 3, 278770])\n",
      "124\n",
      "torch.Size([1, 3, 278770])\n",
      "125\n",
      "torch.Size([1, 3, 278770])\n",
      "126\n",
      "torch.Size([1, 3, 278770])\n",
      "127\n",
      "torch.Size([1, 3, 278770])\n",
      "128\n",
      "torch.Size([1, 3, 278770])\n",
      "129\n",
      "torch.Size([1, 3, 278770])\n",
      "130\n",
      "torch.Size([1, 3, 278770])\n",
      "131\n",
      "torch.Size([1, 3, 278770])\n",
      "132\n",
      "torch.Size([1, 3, 278770])\n",
      "133\n",
      "torch.Size([1, 3, 278770])\n",
      "134\n",
      "torch.Size([1, 3, 278770])\n",
      "135\n",
      "torch.Size([1, 3, 278770])\n",
      "136\n",
      "torch.Size([1, 3, 278770])\n",
      "137\n",
      "torch.Size([1, 3, 278770])\n",
      "138\n",
      "torch.Size([1, 3, 278770])\n",
      "139\n",
      "torch.Size([1, 3, 278770])\n",
      "140\n",
      "torch.Size([1, 3, 278770])\n",
      "141\n",
      "torch.Size([1, 3, 278770])\n",
      "142\n",
      "torch.Size([1, 3, 278770])\n",
      "143\n",
      "torch.Size([1, 3, 278770])\n",
      "144\n",
      "torch.Size([1, 3, 278770])\n",
      "145\n",
      "torch.Size([1, 3, 278770])\n",
      "146\n",
      "torch.Size([1, 3, 278770])\n",
      "147\n",
      "torch.Size([1, 3, 278770])\n",
      "148\n",
      "torch.Size([1, 3, 278770])\n",
      "149\n",
      "torch.Size([1, 3, 278770])\n",
      "150\n",
      "torch.Size([1, 3, 222530])\n",
      "151\n",
      "torch.Size([1, 3, 278770])\n",
      "152\n",
      "torch.Size([1, 3, 278770])\n",
      "153\n",
      "torch.Size([1, 3, 278770])\n",
      "154\n",
      "torch.Size([1, 3, 278770])\n",
      "155\n",
      "torch.Size([1, 3, 278770])\n",
      "156\n",
      "torch.Size([1, 3, 278770])\n",
      "157\n",
      "torch.Size([1, 3, 278770])\n",
      "158\n",
      "torch.Size([1, 3, 278770])\n",
      "159\n",
      "torch.Size([1, 3, 278770])\n",
      "160\n",
      "torch.Size([1, 3, 222530])\n",
      "161\n",
      "torch.Size([1, 3, 278770])\n",
      "162\n",
      "torch.Size([1, 3, 278770])\n",
      "163\n",
      "torch.Size([1, 3, 278770])\n",
      "164\n",
      "torch.Size([1, 3, 278770])\n",
      "165\n",
      "torch.Size([1, 3, 278770])\n",
      "166\n",
      "torch.Size([1, 3, 278770])\n",
      "167\n",
      "torch.Size([1, 3, 278770])\n",
      "168\n",
      "torch.Size([1, 3, 278770])\n",
      "169\n",
      "torch.Size([1, 3, 278770])\n",
      "170\n",
      "torch.Size([1, 3, 278770])\n",
      "171\n",
      "torch.Size([1, 3, 278770])\n",
      "172\n",
      "torch.Size([1, 3, 278770])\n",
      "173\n",
      "torch.Size([1, 3, 278770])\n",
      "174\n",
      "torch.Size([1, 3, 222530])\n",
      "175\n",
      "torch.Size([1, 3, 278770])\n",
      "176\n",
      "torch.Size([1, 3, 278770])\n",
      "177\n",
      "torch.Size([1, 3, 278770])\n",
      "178\n",
      "torch.Size([1, 3, 278770])\n",
      "179\n",
      "torch.Size([1, 3, 278770])\n",
      "180\n",
      "torch.Size([1, 3, 278770])\n",
      "181\n",
      "torch.Size([1, 3, 278770])\n",
      "182\n",
      "torch.Size([1, 3, 278770])\n",
      "183\n",
      "torch.Size([1, 3, 278770])\n",
      "184\n",
      "torch.Size([1, 3, 278770])\n",
      "185\n",
      "tensor([0.3862, 0.4908, 0.2898])\n",
      "tensor([0.1718, 0.1712, 0.1519])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataloader))\n",
    "mean = 0.\n",
    "std = 0.\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "        images, targets = data['image'], data['target']\n",
    "        bs = images.size(0)\n",
    "        images = images.view(bs, images.size(1), -1).float()\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        print(images.size())\n",
    "        print(i) \n",
    "mean /= len(dataloader)\n",
    "std /= len(dataloader)\n",
    "print(mean/255.)\n",
    "print(std/255.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2 as cv\n",
    "from scipy.ndimage import gaussian_filter, morphology\n",
    "from skimage.measure import label, regionprops\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mae(pd, gt):\n",
    "    pd, gt = np.array(pd), np.array(gt)\n",
    "    diff = pd - gt\n",
    "    mae = np.mean(np.abs(diff))\n",
    "    return mae\n",
    "\n",
    "\n",
    "def compute_mse(pd, gt):\n",
    "    pd, gt = np.array(pd), np.array(gt)\n",
    "    diff = pd - gt\n",
    "    mse = np.sqrt(np.mean((diff ** 2)))\n",
    "    return mse\n",
    "\n",
    "def compute_relerr(pd, gt):\n",
    "    pd, gt = np.array(pd), np.array(gt)\n",
    "    diff = pd - gt\n",
    "    diff = diff[gt > 0]\n",
    "    gt = gt[gt > 0]\n",
    "    if (diff is not None) and (gt is not None):\n",
    "        rmae = np.mean(np.abs(diff) / gt) * 100\n",
    "        rmse = np.sqrt(np.mean(diff**2 / gt**2)) * 100\n",
    "    else:\n",
    "        rmae = 0\n",
    "        rmse = 0\n",
    "    return rmae, rmse\n",
    "\n",
    "\n",
    "def rsquared(pd, gt):\n",
    "    \"\"\" Return R^2 where x and y are array-like.\"\"\"\n",
    "    pd, gt = np.array(pd), np.array(gt)\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(pd, gt)\n",
    "    return r_value**2\n",
    "\n",
    "\n",
    "def dense_sample2d(x, sx, stride):\n",
    "    (h,w) = x.shape[:2]\n",
    "    #idx_img = np.array([i for i in range(h*w)]).reshape(h,w)\n",
    "    idx_img = np.zeros((h,w),dtype=float)\n",
    "    \n",
    "    th = [i for i in range(0, h-sx+1, stride)]\n",
    "    tw = [j for j in range(0, w-sx+1, stride)]\n",
    "    norm_vec = np.zeros(len(th)*len(tw))\n",
    "\n",
    "    for i in th:\n",
    "        for j in tw:\n",
    "            idx_img[i:i+sx,j:j+sx] = idx_img[i:i+sx,j:j+sx]+1\n",
    "\n",
    "    # # plot redundancy map\n",
    "    # import os\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # cmap = plt.cm.get_cmap('hot')\n",
    "    # idx_img = idx_img / (idx_img.max())\n",
    "    # idx_img = cmap(idx_img) * 255.\n",
    "    # plt.figure()\n",
    "    # plt.imshow(idx_img.astype(np.uint8))\n",
    "    # plt.axis('off')\n",
    "    # plt.savefig(os.path.join('redundancy_map.pdf'), bbox_inches='tight', dpi = 300)\n",
    "    # plt.close()\n",
    "   \n",
    "    idx_img = 1/idx_img\n",
    "    idx_img = idx_img/sx/sx\n",
    "    #line order\n",
    "    idx = 0\n",
    "    for i in th:\n",
    "        for j in tw:\n",
    "            norm_vec[idx] =idx_img[i:i+sx,j:j+sx].sum()\n",
    "            idx+=1\n",
    "    \n",
    "    return norm_vec\n",
    "\n",
    "\n",
    "def recover_countmap(pred, image, patch_sz, stride):\n",
    "    pred = pred.reshape(-1)\n",
    "    imH, imW = image.shape[2:4]\n",
    "    cntMap = np.zeros((imH, imW), dtype=float)\n",
    "    norMap = np.zeros((imH, imW), dtype=float)\n",
    "    \n",
    "    H = np.arange(0, imH - patch_sz + 1, stride)\n",
    "    W = np.arange(0, imW - patch_sz + 1, stride)\n",
    "    cnt = 0\n",
    "    for h in H:\n",
    "        for w in W:\n",
    "            pixel_cnt = pred[cnt] / patch_sz / patch_sz\n",
    "            cntMap[h:h+patch_sz, w:w+patch_sz] += pixel_cnt\n",
    "            norMap[h:h+patch_sz, w:w+patch_sz] += np.ones((patch_sz,patch_sz))\n",
    "            cnt += 1\n",
    "    return cntMap / (norMap + 1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hlnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, arc='tasselnetv2'):\n",
    "        super(Encoder, self).__init__()\n",
    "        if arc == 'tasselnetv2':\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d((2, 2), stride=2),\n",
    "                nn.Conv2d(16, 32, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d((2, 2), stride=2),\n",
    "                nn.Conv2d(32, 64, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d((2, 2), stride=2),\n",
    "                nn.Conv2d(64, 128, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(128, 128, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        elif arc == 'tasselnetv2plus':\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d((2, 2), stride=2),\n",
    "                nn.Conv2d(16, 32, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d((2, 2), stride=2),\n",
    "                nn.Conv2d(32, 64, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d((2, 2), stride=2),\n",
    "                nn.Conv2d(64, 128, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(128, 128, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counter(nn.Module):\n",
    "    def __init__(self, arc='tasselnetv2', input_size=64, output_stride=8):\n",
    "        super(Counter, self).__init__()\n",
    "        k = int(input_size / 8)\n",
    "        avg_pool_stride = int(output_stride / 8)\n",
    "\n",
    "        if arc == 'tasselnetv2':\n",
    "            self.counter = nn.Sequential(\n",
    "                nn.Conv2d(128, 128, (k, k), bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(128, 128, 1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(128, 1, 1)\n",
    "            )\n",
    "        elif arc == 'tasselnetv2plus':\n",
    "            self.counter = nn.Sequential(\n",
    "                nn.AvgPool2d((k, k), stride=avg_pool_stride),\n",
    "                nn.Conv2d(128, 128, 1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(128, 1, 1)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.counter(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    @staticmethod\n",
    "    def cpu_normalizer(x, imh, imw, insz, os):\n",
    "        # CPU normalization\n",
    "        bs = x.size()[0]\n",
    "        normx = np.zeros((imh, imw))\n",
    "        norm_vec = dense_sample2d(normx, insz, os).astype(np.float32)\n",
    "        x = x.cpu().detach().numpy().reshape(bs, -1) * norm_vec\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def gpu_normalizer(x, imh, imw, insz, os):\n",
    "        _, _, h, w = x.size()            \n",
    "        accm = torch.cuda.FloatTensor(1, insz*insz, h*w).fill_(1)           \n",
    "        accm = F.fold(accm, (imh, imw), kernel_size=insz, stride=os)\n",
    "        accm = 1 / accm\n",
    "        accm /= insz**2\n",
    "        accm = F.unfold(accm, kernel_size=insz, stride=os).sum(1).view(1, 1, h, w)\n",
    "        x *= accm\n",
    "        return x.squeeze().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountingModels(nn.Module):\n",
    "    def __init__(self, arc='tasselnetv2', input_size=64, output_stride=8):\n",
    "        super(CountingModels, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_stride = output_stride\n",
    "\n",
    "        self.encoder = Encoder(arc)\n",
    "        self.counter = Counter(arc, input_size, output_stride)\n",
    "        if arc == 'tasselnetv2':\n",
    "            self.normalizer = Normalizer.cpu_normalizer\n",
    "        elif arc == 'tasselnetv2plus':\n",
    "            self.normalizer = Normalizer.gpu_normalizer\n",
    "        \n",
    "        self.weight_init()\n",
    "\n",
    "    def forward(self, x, is_normalize=True):\n",
    "        imh, imw = x.size()[2:]\n",
    "        x = self.encoder(x)\n",
    "        x = self.counter(x)\n",
    "        if is_normalize:\n",
    "            x = self.normalizer(x, imh, imw, self.input_size, self.output_stride)\n",
    "        return x\n",
    "\n",
    "    def weight_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                # nn.init.kaiming_uniform_(\n",
    "                #         m.weight, \n",
    "                #         mode='fan_in', \n",
    "                #         nonlinearity='relu'\n",
    "                #         )\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-91236e434f60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0minsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mimH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1080\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1920\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountingModels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tasselnetv2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_stride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mcuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    678\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[0mdestination\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkeep_vars\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m         r\"\"\"Returns a dictionary containing a whole state of the module.\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;31m# Support loading old checkpoints that don't have the following attrs:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;31m# Support loading old checkpoints that don't have the following attrs:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 570\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__setstate__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    571\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    572\u001b[0m         \u001b[1;31m# Support loading old checkpoints that don't have the following attrs:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m    594\u001b[0m             type(self).__name__, name))\n\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    678\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[0mdestination\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprefix\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mkeep_vars\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdestination\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m         r\"\"\"Returns a dictionary containing a whole state of the module.\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprev_idx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_setDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[0m_lazy_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "insz, os = 64, 8\n",
    "imH, imW = 1080, 1920\n",
    "net = CountingModels(arc='tasselnetv2', input_size=insz, output_stride=os).cuda()\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    x = torch.randn(1, 3, imH, imW).cuda()\n",
    "    y = net(x)\n",
    "    print(y.shape)    \n",
    "\n",
    "with torch.no_grad():\n",
    "    frame_rate = np.zeros((100, 1))\n",
    "    for i in range(100):\n",
    "        x = torch.randn(1, 3, imH, imW).cuda()\n",
    "        torch.cuda.synchronize()\n",
    "        start = time()\n",
    "\n",
    "        y = net(x)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end = time()\n",
    "\n",
    "        running_frame_rate = 1 * float(1 / (end - start))\n",
    "        frame_rate[i] = running_frame_rate\n",
    "    print(np.mean(frame_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hltrainval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from time import time\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "# from skimage.measure import compare_psnr updated to peak_signal_noise_ratio\n",
    "# from skimage.measure import compare_ssim updated to structural_similarity\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from skimage.metrics import structural_similarity\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent dataloader deadlock, uncomment if deadlock occurs\n",
    "# cv.setNumThreads(0)\n",
    "cudnn.enabled = True\n",
    "\n",
    "# constant\n",
    "IMG_SCALE = 1. / 255\n",
    "IMG_MEAN = [.3405, .4747, .2418]\n",
    "IMG_STD = [1, 1, 1]\n",
    "SCALES = [0.7, 1, 1.3]\n",
    "SHORTER_SIDE = 224\n",
    "\n",
    "# system-related parameters\n",
    "DATA_DIR = 'maize_counting_dataset'\n",
    "DATASET = 'mtc'\n",
    "EXP = 'tasselnetv2_rf110_i64o8_r0125_crop256_lr-2_bs9_epoch500'\n",
    "DATA_LIST = 'maize_counting_dataset/train.txt'\n",
    "DATA_VAL_LIST = 'maize_counting_dataset/test.txt'\n",
    "\n",
    "RESTORE_FROM = 'model_best.pth.tar'\n",
    "SNAPSHOT_DIR = './snapshots'\n",
    "RESULT_DIR = './results'\n",
    "\n",
    "# model-related parameters\n",
    "INPUT_SIZE = 64\n",
    "OUTPUT_STRIDE = 8\n",
    "MODEL = 'tasselnetv2'\n",
    "RESIZE_RATIO = 0.125\n",
    "\n",
    "# training-related parameters\n",
    "OPTIMIZER = 'sgd'  # choice in ['sgd', 'adam']\n",
    "BATCH_SIZE = 9\n",
    "CROP_SIZE = (256, 256)\n",
    "LEARNING_RATE = 1e-2\n",
    "MILESTONES = [200, 400]\n",
    "MOMENTUM = 0.95\n",
    "MULT = 1\n",
    "NUM_EPOCHS = 500\n",
    "NUM_CPU_WORKERS = 0\n",
    "PRINT_EVERY = 1\n",
    "RANDOM_SEED = 6\n",
    "WEIGHT_DECAY = 5e-4\n",
    "VAL_EVERY = 1\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# add a new entry here if creating a new data loader\n",
    "dataset_list = {\n",
    "    'mtc': MaizeTasselDataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
