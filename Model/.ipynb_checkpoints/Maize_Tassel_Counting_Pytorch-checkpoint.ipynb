{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cu113/torch_stable.htmlNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Collecting torch==1.10.0+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torch-1.10.0%2Bcu113-cp38-cp38-win_amd64.whl (2442.4 MB)\n",
      "Collecting torchvision==0.11.1+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torchvision-0.11.1%2Bcu113-cp38-cp38-win_amd64.whl (3.2 MB)\n",
      "Collecting torchaudio===0.10.0+cu113\n",
      "  Downloading https://download.pytorch.org/whl/cu113/torchaudio-0.10.0%2Bcu113-cp38-cp38-win_amd64.whl (336 kB)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda3\\lib\\site-packages (from torch==1.10.0+cu113) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in d:\\anaconda3\\lib\\site-packages (from torchvision==0.11.1+cu113) (1.19.2)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in d:\\anaconda3\\lib\\site-packages (from torchvision==0.11.1+cu113) (8.0.1)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.4.0+cu92\n",
      "    Uninstalling torch-1.4.0+cu92:\n",
      "      Successfully uninstalled torch-1.4.0+cu92\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.11.1\n",
      "    Uninstalling torchvision-0.11.1:\n",
      "      Successfully uninstalled torchvision-0.11.1\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 0.10.0\n",
      "    Uninstalling torchaudio-0.10.0:\n",
      "      Successfully uninstalled torchaudio-0.10.0\n",
      "Successfully installed torch-1.10.0+cu113 torchaudio-0.10.0+cu113 torchvision-0.11.1+cu113\n"
     ]
    }
   ],
   "source": [
    "#pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio===0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9379, 0.7414, 0.2243],\n",
      "        [0.7988, 0.0589, 0.6338],\n",
      "        [0.2547, 0.3420, 0.9909],\n",
      "        [0.6532, 0.3251, 0.8231],\n",
      "        [0.9152, 0.9127, 0.9594]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gen_trainval_list.py\n",
    "\n",
    "This portion of the code seems to be only writing the image name and its corresponding mat files into a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = './maize_counting_dataset'\n",
    "image_folder = 'images'\n",
    "label_folder = 'labels'\n",
    "train = 'train'\n",
    "val = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(root, train)\n",
    "with open('train.txt', 'w') as f:\n",
    "    for image_path in glob.glob(os.path.join(train_path, image_folder, '*.JPG')):\n",
    "        im_path = image_path.replace(root, '')\n",
    "        gt_path = im_path.replace(image_folder, label_folder).replace('.JPG', '.xml')\n",
    "        f.write(im_path+'\\t'+gt_path+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_path = os.path.join(root, val)\n",
    "with open('val.txt', 'w') as f:\n",
    "    for image_path in glob.glob(os.path.join(val_path, image_folder, '*.JPG')):\n",
    "        im_path = image_path.replace(root, '')\n",
    "        gt_path = im_path.replace(image_folder, label_folder).replace('.JPG', '.xml')\n",
    "        f.write(im_path+'\\t'+gt_path+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hldataset.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd \n",
    "import random\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import h5py\n",
    "import scipy.io as sio\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from skimage import util\n",
    "from skimage.measure import label\n",
    "from skimage.measure import regionprops\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(x):\n",
    "    img_arr = np.array(Image.open(x))\n",
    "    if len(img_arr.shape) == 2:  # grayscale\n",
    "        img_arr = np.tile(img_arr, [3, 1, 1]).transpose(1, 2, 0)\n",
    "    return img_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomCrop(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "\n",
    "        image, target, gtcount = sample['image'], sample['target'], sample['gtcount']\n",
    "        h, w = image.shape[:2]\n",
    "\n",
    "        if isinstance(self.output_size, tuple):\n",
    "            new_h = min(self.output_size[0], h)\n",
    "            new_w = min(self.output_size[1], w)\n",
    "            assert (new_h, new_w) == self.output_size\n",
    "        else:\n",
    "            crop_size = min(self.output_size, h, w)\n",
    "            assert crop_size == self.output_size\n",
    "            new_h = new_w = crop_size\n",
    "        if gtcount > 0:\n",
    "            mask = target > 0\n",
    "            ch, cw = int(np.ceil(new_h / 2)), int(np.ceil(new_w / 2))\n",
    "            mask_center = np.zeros((h, w), dtype=np.uint8)\n",
    "            mask_center[ch:h-ch+1, cw:w-cw+1] = 1\n",
    "            mask = (mask & mask_center)\n",
    "            idh, idw = np.where(mask == 1)\n",
    "            if len(idh) != 0:\n",
    "                ids = random.choice(range(len(idh)))\n",
    "                hc, wc = idh[ids], idw[ids]\n",
    "                top, left = hc-ch, wc-cw\n",
    "            else:\n",
    "                top = np.random.randint(0, h-new_h+1)\n",
    "                left = np.random.randint(0, w-new_w+1)\n",
    "        else:\n",
    "            top = np.random.randint(0, h-new_h+1)\n",
    "            left = np.random.randint(0, w-new_w+1)\n",
    "\n",
    "        image = image[top:top+new_h, left:left+new_w, :]\n",
    "        target = target[top:top+new_h, left:left+new_w]\n",
    "\n",
    "        return {'image': image, 'target': target, 'gtcount': gtcount}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomFlip(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, target, gtcount = sample['image'], sample['target'], sample['gtcount']\n",
    "        do_mirror = np.random.randint(2)\n",
    "        if do_mirror:\n",
    "            image = cv2.flip(image, 1)\n",
    "            target = cv2.flip(target, 1)\n",
    "        return {'image': image, 'target': target, 'gtcount': gtcount}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "\n",
    "    def __init__(self, scale, mean, std):\n",
    "        self.scale = scale\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, target, gtcount = sample['image'], sample['target'], sample['gtcount']\n",
    "        image, target = image.astype('float32'), target.astype('float32')\n",
    "\n",
    "        # pixel normalization\n",
    "        image = (self.scale * image - self.mean) / self.std\n",
    "\n",
    "        image, target = image.astype('float32'), target.astype('float32')\n",
    "\n",
    "        return {'image': image, 'target': target, 'gtcount': gtcount}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ZeroPadding(object):\n",
    "    def __init__(self, psize=32):\n",
    "        self.psize = psize\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        psize =  self.psize\n",
    "\n",
    "        image, target, gtcount = sample['image'], sample['target'], sample['gtcount']\n",
    "        h,w = image.size()[-2:]\n",
    "        ph,pw = (psize-h%psize),(psize-w%psize)\n",
    "        # print(ph,pw)\n",
    "\n",
    "        (pl, pr) = (pw//2, pw-pw//2) if pw != psize else (0, 0)\n",
    "        (pt, pb) = (ph//2, ph-ph//2) if ph != psize else (0, 0)\n",
    "        if (ph!=psize) or (pw!=psize):\n",
    "            tmp_pad = [pl, pr, pt, pb]\n",
    "            # print(tmp_pad)\n",
    "            image = F.pad(image,tmp_pad)\n",
    "            target = F.pad(target,tmp_pad)\n",
    "\n",
    "        return {'image': image, 'target': target, 'gtcount': gtcount}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # swap color axis\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        image, target, gtcount = sample['image'], sample['target'], sample['gtcount']\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        target = np.expand_dims(target, axis=2)\n",
    "        target = target.transpose((2, 0, 1))\n",
    "        image, target = torch.from_numpy(image), torch.from_numpy(target)\n",
    "        return {'image': image, 'target': target, 'gtcount': gtcount}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaizeTasselDataset(Dataset):\n",
    "    def __init__(self, data_dir, data_list, ratio, train=True, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.data_list = [name.split('\\t') for name in open(data_list).read().splitlines()]\n",
    "        self.ratio = ratio\n",
    "        self.train = train\n",
    "        self.transform = transform\n",
    "        self.image_list = []\n",
    "        \n",
    "        # store images and generate ground truths\n",
    "        self.images = {}\n",
    "        self.targets = {}\n",
    "        self.gtcounts = {}\n",
    "        self.dotimages = {}\n",
    "\n",
    "    def bbs2points(self, bbs):    \n",
    "        points = []\n",
    "        for bb in bbs:\n",
    "            x1, y1, w, h = [float(b) for b in bb]\n",
    "            x2, y2 = x1+w-1, y1+h-1\n",
    "            x, y = np.round((x1+x2)/2).astype(np.int32), np.round((y1+y2)/2).astype(np.int32)\n",
    "            points.append([x, y])\n",
    "        return points\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_name = self.data_list[idx]\n",
    "        self.image_list.append(file_name[0])\n",
    "        if file_name[0] not in self.images:\n",
    "            image = read_image(self.data_dir+file_name[0])\n",
    "            annotation = sio.loadmat(self.data_dir+file_name[1])\n",
    "            h, w = image.shape[:2]\n",
    "            nh = int(np.ceil(h * self.ratio))\n",
    "            nw = int(np.ceil(w * self.ratio))\n",
    "            image = cv2.resize(image, (nw, nh), interpolation = cv2.INTER_CUBIC)\n",
    "            target = np.zeros((nh, nw), dtype=np.float32)\n",
    "            dotimage = image.copy()\n",
    "            if annotation['annotation'][0][0][1] is not None:\n",
    "                bbs = annotation['annotation'][0][0][1]\n",
    "                gtcount = bbs.shape[0]\n",
    "                pts = self.bbs2points(bbs)\n",
    "                for pt in pts:\n",
    "                    pt[0], pt[1] = int(pt[0] * self.ratio), int(pt[1] * self.ratio)\n",
    "                    target[pt[1], pt[0]] = 1\n",
    "                    cv2.circle(dotimage, (pt[0], pt[1]), int(24 * self.ratio) , (255, 0, 0), -1)\n",
    "            else:\n",
    "                gtcount = 0\n",
    "            target = gaussian_filter(target, 80 * self.ratio)\n",
    "\n",
    "            # plt.imshow(target, cmap=cm.jet)\n",
    "            # plt.show()\n",
    "            # print(target.sum())\n",
    "\n",
    "            self.images.update({file_name[0]:image})\n",
    "            self.targets.update({file_name[0]:target})\n",
    "            self.gtcounts.update({file_name[0]:gtcount})\n",
    "            self.dotimages.update({file_name[0]:dotimage})\n",
    "\n",
    "        \n",
    "        sample = {\n",
    "            'image': self.images[file_name[0]], \n",
    "            'target': self.targets[file_name[0]], \n",
    "            'gtcount': self.gtcounts[file_name[0]]\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MaizeTasselDataset(\n",
    "        data_dir='maize_counting_dataset', \n",
    "        data_list='maize_counting_dataset/train.txt',\n",
    "        ratio=0.167, \n",
    "        train=True, \n",
    "        transform=transforms.Compose([\n",
    "            ToTensor()]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n",
      "torch.Size([1, 3, 278770])\n",
      "0\n",
      "torch.Size([1, 3, 278770])\n",
      "1\n",
      "torch.Size([1, 3, 278770])\n",
      "2\n",
      "torch.Size([1, 3, 278770])\n",
      "3\n",
      "torch.Size([1, 3, 278770])\n",
      "4\n",
      "torch.Size([1, 3, 222530])\n",
      "5\n",
      "torch.Size([1, 3, 278770])\n",
      "6\n",
      "torch.Size([1, 3, 222530])\n",
      "7\n",
      "torch.Size([1, 3, 278770])\n",
      "8\n",
      "torch.Size([1, 3, 278770])\n",
      "9\n",
      "torch.Size([1, 3, 278770])\n",
      "10\n",
      "torch.Size([1, 3, 278770])\n",
      "11\n",
      "torch.Size([1, 3, 278770])\n",
      "12\n",
      "torch.Size([1, 3, 278770])\n",
      "13\n",
      "torch.Size([1, 3, 278770])\n",
      "14\n",
      "torch.Size([1, 3, 278770])\n",
      "15\n",
      "torch.Size([1, 3, 278770])\n",
      "16\n",
      "torch.Size([1, 3, 278770])\n",
      "17\n",
      "torch.Size([1, 3, 278770])\n",
      "18\n",
      "torch.Size([1, 3, 278770])\n",
      "19\n",
      "torch.Size([1, 3, 278770])\n",
      "20\n",
      "torch.Size([1, 3, 278770])\n",
      "21\n",
      "torch.Size([1, 3, 278770])\n",
      "22\n",
      "torch.Size([1, 3, 278770])\n",
      "23\n",
      "torch.Size([1, 3, 278770])\n",
      "24\n",
      "torch.Size([1, 3, 278770])\n",
      "25\n",
      "torch.Size([1, 3, 278770])\n",
      "26\n",
      "torch.Size([1, 3, 278770])\n",
      "27\n",
      "torch.Size([1, 3, 278770])\n",
      "28\n",
      "torch.Size([1, 3, 278770])\n",
      "29\n",
      "torch.Size([1, 3, 278770])\n",
      "30\n",
      "torch.Size([1, 3, 278770])\n",
      "31\n",
      "torch.Size([1, 3, 278770])\n",
      "32\n",
      "torch.Size([1, 3, 278770])\n",
      "33\n",
      "torch.Size([1, 3, 278770])\n",
      "34\n",
      "torch.Size([1, 3, 278770])\n",
      "35\n",
      "torch.Size([1, 3, 278770])\n",
      "36\n",
      "torch.Size([1, 3, 278770])\n",
      "37\n",
      "torch.Size([1, 3, 278770])\n",
      "38\n",
      "torch.Size([1, 3, 278770])\n",
      "39\n",
      "torch.Size([1, 3, 278770])\n",
      "40\n",
      "torch.Size([1, 3, 278770])\n",
      "41\n",
      "torch.Size([1, 3, 278770])\n",
      "42\n",
      "torch.Size([1, 3, 278770])\n",
      "43\n",
      "torch.Size([1, 3, 278770])\n",
      "44\n",
      "torch.Size([1, 3, 278770])\n",
      "45\n",
      "torch.Size([1, 3, 278770])\n",
      "46\n",
      "torch.Size([1, 3, 222530])\n",
      "47\n",
      "torch.Size([1, 3, 278770])\n",
      "48\n",
      "torch.Size([1, 3, 278770])\n",
      "49\n",
      "torch.Size([1, 3, 278770])\n",
      "50\n",
      "torch.Size([1, 3, 278770])\n",
      "51\n",
      "torch.Size([1, 3, 278770])\n",
      "52\n",
      "torch.Size([1, 3, 278770])\n",
      "53\n",
      "torch.Size([1, 3, 278770])\n",
      "54\n",
      "torch.Size([1, 3, 278770])\n",
      "55\n",
      "torch.Size([1, 3, 278770])\n",
      "56\n",
      "torch.Size([1, 3, 278770])\n",
      "57\n",
      "torch.Size([1, 3, 278770])\n",
      "58\n",
      "torch.Size([1, 3, 278770])\n",
      "59\n",
      "torch.Size([1, 3, 278770])\n",
      "60\n",
      "torch.Size([1, 3, 222530])\n",
      "61\n",
      "torch.Size([1, 3, 278770])\n",
      "62\n",
      "torch.Size([1, 3, 278770])\n",
      "63\n",
      "torch.Size([1, 3, 278770])\n",
      "64\n",
      "torch.Size([1, 3, 278770])\n",
      "65\n",
      "torch.Size([1, 3, 278770])\n",
      "66\n",
      "torch.Size([1, 3, 222530])\n",
      "67\n",
      "torch.Size([1, 3, 278770])\n",
      "68\n",
      "torch.Size([1, 3, 278770])\n",
      "69\n",
      "torch.Size([1, 3, 278770])\n",
      "70\n",
      "torch.Size([1, 3, 278770])\n",
      "71\n",
      "torch.Size([1, 3, 278770])\n",
      "72\n",
      "torch.Size([1, 3, 278770])\n",
      "73\n",
      "torch.Size([1, 3, 278770])\n",
      "74\n",
      "torch.Size([1, 3, 222530])\n",
      "75\n",
      "torch.Size([1, 3, 278770])\n",
      "76\n",
      "torch.Size([1, 3, 278770])\n",
      "77\n",
      "torch.Size([1, 3, 278770])\n",
      "78\n",
      "torch.Size([1, 3, 278770])\n",
      "79\n",
      "torch.Size([1, 3, 278770])\n",
      "80\n",
      "torch.Size([1, 3, 278770])\n",
      "81\n",
      "torch.Size([1, 3, 278770])\n",
      "82\n",
      "torch.Size([1, 3, 278770])\n",
      "83\n",
      "torch.Size([1, 3, 278770])\n",
      "84\n",
      "torch.Size([1, 3, 278770])\n",
      "85\n",
      "torch.Size([1, 3, 278770])\n",
      "86\n",
      "torch.Size([1, 3, 278770])\n",
      "87\n",
      "torch.Size([1, 3, 278770])\n",
      "88\n",
      "torch.Size([1, 3, 278770])\n",
      "89\n",
      "torch.Size([1, 3, 278770])\n",
      "90\n",
      "torch.Size([1, 3, 278770])\n",
      "91\n",
      "torch.Size([1, 3, 222530])\n",
      "92\n",
      "torch.Size([1, 3, 278770])\n",
      "93\n",
      "torch.Size([1, 3, 278770])\n",
      "94\n",
      "torch.Size([1, 3, 278770])\n",
      "95\n",
      "torch.Size([1, 3, 222530])\n",
      "96\n",
      "torch.Size([1, 3, 278770])\n",
      "97\n",
      "torch.Size([1, 3, 278770])\n",
      "98\n",
      "torch.Size([1, 3, 278770])\n",
      "99\n",
      "torch.Size([1, 3, 278770])\n",
      "100\n",
      "torch.Size([1, 3, 278770])\n",
      "101\n",
      "torch.Size([1, 3, 278770])\n",
      "102\n",
      "torch.Size([1, 3, 278770])\n",
      "103\n",
      "torch.Size([1, 3, 278770])\n",
      "104\n",
      "torch.Size([1, 3, 278770])\n",
      "105\n",
      "torch.Size([1, 3, 278770])\n",
      "106\n",
      "torch.Size([1, 3, 278770])\n",
      "107\n",
      "torch.Size([1, 3, 278770])\n",
      "108\n",
      "torch.Size([1, 3, 278770])\n",
      "109\n",
      "torch.Size([1, 3, 278770])\n",
      "110\n",
      "torch.Size([1, 3, 278770])\n",
      "111\n",
      "torch.Size([1, 3, 278770])\n",
      "112\n",
      "torch.Size([1, 3, 222530])\n",
      "113\n",
      "torch.Size([1, 3, 278770])\n",
      "114\n",
      "torch.Size([1, 3, 278770])\n",
      "115\n",
      "torch.Size([1, 3, 278770])\n",
      "116\n",
      "torch.Size([1, 3, 278770])\n",
      "117\n",
      "torch.Size([1, 3, 278770])\n",
      "118\n",
      "torch.Size([1, 3, 278770])\n",
      "119\n",
      "torch.Size([1, 3, 278770])\n",
      "120\n",
      "torch.Size([1, 3, 278770])\n",
      "121\n",
      "torch.Size([1, 3, 278770])\n",
      "122\n",
      "torch.Size([1, 3, 278770])\n",
      "123\n",
      "torch.Size([1, 3, 278770])\n",
      "124\n",
      "torch.Size([1, 3, 278770])\n",
      "125\n",
      "torch.Size([1, 3, 278770])\n",
      "126\n",
      "torch.Size([1, 3, 278770])\n",
      "127\n",
      "torch.Size([1, 3, 278770])\n",
      "128\n",
      "torch.Size([1, 3, 278770])\n",
      "129\n",
      "torch.Size([1, 3, 278770])\n",
      "130\n",
      "torch.Size([1, 3, 278770])\n",
      "131\n",
      "torch.Size([1, 3, 278770])\n",
      "132\n",
      "torch.Size([1, 3, 278770])\n",
      "133\n",
      "torch.Size([1, 3, 278770])\n",
      "134\n",
      "torch.Size([1, 3, 278770])\n",
      "135\n",
      "torch.Size([1, 3, 278770])\n",
      "136\n",
      "torch.Size([1, 3, 278770])\n",
      "137\n",
      "torch.Size([1, 3, 278770])\n",
      "138\n",
      "torch.Size([1, 3, 278770])\n",
      "139\n",
      "torch.Size([1, 3, 278770])\n",
      "140\n",
      "torch.Size([1, 3, 278770])\n",
      "141\n",
      "torch.Size([1, 3, 278770])\n",
      "142\n",
      "torch.Size([1, 3, 278770])\n",
      "143\n",
      "torch.Size([1, 3, 278770])\n",
      "144\n",
      "torch.Size([1, 3, 278770])\n",
      "145\n",
      "torch.Size([1, 3, 278770])\n",
      "146\n",
      "torch.Size([1, 3, 278770])\n",
      "147\n",
      "torch.Size([1, 3, 278770])\n",
      "148\n",
      "torch.Size([1, 3, 278770])\n",
      "149\n",
      "torch.Size([1, 3, 278770])\n",
      "150\n",
      "torch.Size([1, 3, 222530])\n",
      "151\n",
      "torch.Size([1, 3, 278770])\n",
      "152\n",
      "torch.Size([1, 3, 278770])\n",
      "153\n",
      "torch.Size([1, 3, 278770])\n",
      "154\n",
      "torch.Size([1, 3, 278770])\n",
      "155\n",
      "torch.Size([1, 3, 278770])\n",
      "156\n",
      "torch.Size([1, 3, 278770])\n",
      "157\n",
      "torch.Size([1, 3, 278770])\n",
      "158\n",
      "torch.Size([1, 3, 278770])\n",
      "159\n",
      "torch.Size([1, 3, 278770])\n",
      "160\n",
      "torch.Size([1, 3, 222530])\n",
      "161\n",
      "torch.Size([1, 3, 278770])\n",
      "162\n",
      "torch.Size([1, 3, 278770])\n",
      "163\n",
      "torch.Size([1, 3, 278770])\n",
      "164\n",
      "torch.Size([1, 3, 278770])\n",
      "165\n",
      "torch.Size([1, 3, 278770])\n",
      "166\n",
      "torch.Size([1, 3, 278770])\n",
      "167\n",
      "torch.Size([1, 3, 278770])\n",
      "168\n",
      "torch.Size([1, 3, 278770])\n",
      "169\n",
      "torch.Size([1, 3, 278770])\n",
      "170\n",
      "torch.Size([1, 3, 278770])\n",
      "171\n",
      "torch.Size([1, 3, 278770])\n",
      "172\n",
      "torch.Size([1, 3, 278770])\n",
      "173\n",
      "torch.Size([1, 3, 278770])\n",
      "174\n",
      "torch.Size([1, 3, 222530])\n",
      "175\n",
      "torch.Size([1, 3, 278770])\n",
      "176\n",
      "torch.Size([1, 3, 278770])\n",
      "177\n",
      "torch.Size([1, 3, 278770])\n",
      "178\n",
      "torch.Size([1, 3, 278770])\n",
      "179\n",
      "torch.Size([1, 3, 278770])\n",
      "180\n",
      "torch.Size([1, 3, 278770])\n",
      "181\n",
      "torch.Size([1, 3, 278770])\n",
      "182\n",
      "torch.Size([1, 3, 278770])\n",
      "183\n",
      "torch.Size([1, 3, 278770])\n",
      "184\n",
      "torch.Size([1, 3, 278770])\n",
      "185\n",
      "tensor([0.3862, 0.4908, 0.2898])\n",
      "tensor([0.1718, 0.1712, 0.1519])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataloader))\n",
    "mean = 0.\n",
    "std = 0.\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "        images, targets = data['image'], data['target']\n",
    "        bs = images.size(0)\n",
    "        images = images.view(bs, images.size(1), -1).float()\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        print(images.size())\n",
    "        print(i) \n",
    "mean /= len(dataloader)\n",
    "std /= len(dataloader)\n",
    "print(mean/255.)\n",
    "print(std/255.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "import math\n",
    "import cv2 as cv\n",
    "from scipy.ndimage import gaussian_filter, morphology\n",
    "from skimage.measure import label, regionprops\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mae(pd, gt):\n",
    "    pd, gt = np.array(pd), np.array(gt)\n",
    "    diff = pd - gt\n",
    "    mae = np.mean(np.abs(diff))\n",
    "    return mae\n",
    "\n",
    "\n",
    "def compute_mse(pd, gt):\n",
    "    pd, gt = np.array(pd), np.array(gt)\n",
    "    diff = pd - gt\n",
    "    mse = np.sqrt(np.mean((diff ** 2)))\n",
    "    return mse\n",
    "\n",
    "def compute_relerr(pd, gt):\n",
    "    pd, gt = np.array(pd), np.array(gt)\n",
    "    diff = pd - gt\n",
    "    diff = diff[gt > 0]\n",
    "    gt = gt[gt > 0]\n",
    "    if (diff is not None) and (gt is not None):\n",
    "        rmae = np.mean(np.abs(diff) / gt) * 100\n",
    "        rmse = np.sqrt(np.mean(diff**2 / gt**2)) * 100\n",
    "    else:\n",
    "        rmae = 0\n",
    "        rmse = 0\n",
    "    return rmae, rmse\n",
    "\n",
    "\n",
    "def rsquared(pd, gt):\n",
    "    \"\"\" Return R^2 where x and y are array-like.\"\"\"\n",
    "    pd, gt = np.array(pd), np.array(gt)\n",
    "    slope, intercept, r_value, p_value, std_err = scipy.stats.linregress(pd, gt)\n",
    "    return r_value**2\n",
    "\n",
    "\n",
    "def dense_sample2d(x, sx, stride):\n",
    "    (h,w) = x.shape[:2]\n",
    "    #idx_img = np.array([i for i in range(h*w)]).reshape(h,w)\n",
    "    idx_img = np.zeros((h,w),dtype=float)\n",
    "    \n",
    "    th = [i for i in range(0, h-sx+1, stride)]\n",
    "    tw = [j for j in range(0, w-sx+1, stride)]\n",
    "    norm_vec = np.zeros(len(th)*len(tw))\n",
    "\n",
    "    for i in th:\n",
    "        for j in tw:\n",
    "            idx_img[i:i+sx,j:j+sx] = idx_img[i:i+sx,j:j+sx]+1\n",
    "\n",
    "    # plot redundancy map\n",
    "    # import os\n",
    "    # import matplotlib.pyplot as plt\n",
    "    # cmap = plt.cm.get_cmap('hot')\n",
    "    # idx_img = idx_img / (idx_img.max())\n",
    "    # idx_img = cmap(idx_img) * 255.\n",
    "    # plt.figure()\n",
    "    # plt.imshow(idx_img.astype(np.uint8))\n",
    "    # plt.axis('off')\n",
    "    # plt.savefig(os.path.join('redundancy_map.pdf'), bbox_inches='tight', dpi = 300)\n",
    "    # plt.close()\n",
    "   \n",
    "    idx_img = 1/idx_img\n",
    "    idx_img = idx_img/sx/sx\n",
    "    #line order\n",
    "    idx = 0\n",
    "    for i in th:\n",
    "        for j in tw:\n",
    "            norm_vec[idx] =idx_img[i:i+sx,j:j+sx].sum()\n",
    "            idx+=1\n",
    "    \n",
    "    return norm_vec\n",
    "\n",
    "\n",
    "def recover_countmap(pred, image, patch_sz, stride):\n",
    "    pred = pred.reshape(-1)\n",
    "    imH, imW = image.shape[2:4]\n",
    "    cntMap = np.zeros((imH, imW), dtype=float)\n",
    "    norMap = np.zeros((imH, imW), dtype=float)\n",
    "    \n",
    "    H = np.arange(0, imH - patch_sz + 1, stride)\n",
    "    W = np.arange(0, imW - patch_sz + 1, stride)\n",
    "    cnt = 0\n",
    "    for h in H:\n",
    "        for w in W:\n",
    "            pixel_cnt = pred[cnt] / patch_sz / patch_sz\n",
    "            cntMap[h:h+patch_sz, w:w+patch_sz] += pixel_cnt\n",
    "            norMap[h:h+patch_sz, w:w+patch_sz] += np.ones((patch_sz,patch_sz))\n",
    "            cnt += 1\n",
    "    return cntMap / (norMap + 1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hlnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, arc='tasselnetv2'):\n",
    "        super(Encoder, self).__init__()\n",
    "        if arc == 'tasselnetv2':\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d((2, 2), stride=2),\n",
    "                nn.Conv2d(16, 32, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d((2, 2), stride=2),\n",
    "                nn.Conv2d(32, 64, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d((2, 2), stride=2),\n",
    "                nn.Conv2d(64, 128, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(128, 128, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        elif arc == 'tasselnetv2plus':\n",
    "            self.encoder = nn.Sequential(\n",
    "                nn.Conv2d(3, 16, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(16),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d((2, 2), stride=2),\n",
    "                nn.Conv2d(16, 32, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d((2, 2), stride=2),\n",
    "                nn.Conv2d(32, 64, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d((2, 2), stride=2),\n",
    "                nn.Conv2d(64, 128, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(128, 128, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Counter(nn.Module):\n",
    "    def __init__(self, arc='tasselnetv2', input_size=64, output_stride=8):\n",
    "        super(Counter, self).__init__()\n",
    "        k = int(input_size / 8)\n",
    "        avg_pool_stride = int(output_stride / 8)\n",
    "\n",
    "        if arc == 'tasselnetv2':\n",
    "            self.counter = nn.Sequential(\n",
    "                nn.Conv2d(128, 128, (k, k), bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(128, 128, 1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(128, 1, 1)\n",
    "            )\n",
    "        elif arc == 'tasselnetv2plus':\n",
    "            self.counter = nn.Sequential(\n",
    "                nn.AvgPool2d((k, k), stride=avg_pool_stride),\n",
    "                nn.Conv2d(128, 128, 1, bias=False),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(128, 1, 1)\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.counter(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer:\n",
    "    @staticmethod\n",
    "    def cpu_normalizer(x, imh, imw, insz, os):\n",
    "        # CPU normalization\n",
    "        bs = x.size()[0]\n",
    "        normx = np.zeros((imh, imw))\n",
    "        norm_vec = dense_sample2d(normx, insz, os).astype(np.float32)\n",
    "        x = x.cpu().detach().numpy().reshape(bs, -1) * norm_vec\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def gpu_normalizer(x, imh, imw, insz, os):\n",
    "        _, _, h, w = x.size()            \n",
    "        accm = torch.cuda.FloatTensor(1, insz*insz, h*w).fill_(1)           \n",
    "        accm = F.fold(accm, (imh, imw), kernel_size=insz, stride=os)\n",
    "        accm = 1 / accm\n",
    "        accm /= insz**2\n",
    "        accm = F.unfold(accm, kernel_size=insz, stride=os).sum(1).view(1, 1, h, w)\n",
    "        x *= accm\n",
    "        return x.squeeze().cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CountingModels(nn.Module):\n",
    "    def __init__(self, arc='tasselnetv2', input_size=64, output_stride=8):\n",
    "        super(CountingModels, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_stride = output_stride\n",
    "\n",
    "        self.encoder = Encoder(arc)\n",
    "        self.counter = Counter(arc, input_size, output_stride)\n",
    "        if arc == 'tasselnetv2':\n",
    "            self.normalizer = Normalizer.cpu_normalizer\n",
    "        elif arc == 'tasselnetv2plus':\n",
    "            self.normalizer = Normalizer.gpu_normalizer\n",
    "        \n",
    "        self.weight_init()\n",
    "\n",
    "    def forward(self, x, is_normalize=True):\n",
    "        imh, imw = x.size()[2:]\n",
    "        x = self.encoder(x)\n",
    "        x = self.counter(x)\n",
    "        if is_normalize:\n",
    "            x = self.normalizer(x, imh, imw, self.input_size, self.output_stride)\n",
    "        return x\n",
    "\n",
    "    def weight_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.normal_(m.weight, std=0.01)\n",
    "                # nn.init.kaiming_uniform_(\n",
    "                #         m.weight, \n",
    "                #         mode='fan_in', \n",
    "                #         nonlinearity='relu'\n",
    "                #         )\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 29824)\n",
      "0.7424600683744045\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "insz, os = 64, 8\n",
    "imH, imW = 1080, 1920\n",
    "#net = CountingModels(arc='tasselnetv2', input_size=insz, output_stride=os).cuda()\n",
    "net = CountingModels(arc='tasselnetv2', input_size=insz, output_stride=os)\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    #x = torch.randn(1, 3, imH, imW).cuda()\n",
    "    x = torch.randn(1, 3, imH, imW)\n",
    "    y = net(x)\n",
    "    print(y.shape)    \n",
    "\n",
    "with torch.no_grad():\n",
    "    frame_rate = np.zeros((100, 1))\n",
    "    for i in range(100):\n",
    "        #x = torch.randn(1, 3, imH, imW).cuda()\n",
    "        x = torch.randn(1, 3, imH, imW)\n",
    "        #torch.cuda.synchronize()\n",
    "        start = time()\n",
    "\n",
    "        y = net(x)\n",
    "\n",
    "        #torch.cuda.synchronize()\n",
    "        end = time()\n",
    "\n",
    "        running_frame_rate = 1 * float(1 / (end - start))\n",
    "        frame_rate[i] = running_frame_rate\n",
    "    print(np.mean(frame_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## hltrainval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from time import time\n",
    "\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "# from skimage.measure import compare_psnr updated to peak_signal_noise_ratio\n",
    "# from skimage.measure import compare_ssim updated to structural_similarity\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "from skimage.metrics import structural_similarity\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR, MultiStepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prevent dataloader deadlock, uncomment if deadlock occurs\n",
    "# cv.setNumThreads(0)\n",
    "cudnn.enabled = True\n",
    "\n",
    "# constant\n",
    "IMG_SCALE = 1. / 255\n",
    "IMG_MEAN = [.3405, .4747, .2418]\n",
    "IMG_STD = [1, 1, 1]\n",
    "SCALES = [0.7, 1, 1.3]\n",
    "SHORTER_SIDE = 224\n",
    "\n",
    "# system-related parameters\n",
    "DATA_DIR = 'maize_counting_dataset'\n",
    "DATASET = 'mtc'\n",
    "EXP = 'tasselnetv2_rf110_i64o8_r0125_crop256_lr-2_bs9_epoch500'\n",
    "DATA_LIST = 'maize_counting_dataset/train.txt'\n",
    "DATA_VAL_LIST = 'maize_counting_dataset/test.txt'\n",
    "\n",
    "RESTORE_FROM = 'model_best.pth.tar'\n",
    "SNAPSHOT_DIR = './snapshots'\n",
    "RESULT_DIR = './results'\n",
    "\n",
    "# model-related parameters\n",
    "INPUT_SIZE = 64\n",
    "OUTPUT_STRIDE = 8\n",
    "MODEL = 'tasselnetv2'\n",
    "RESIZE_RATIO = 0.125\n",
    "\n",
    "# training-related parameters\n",
    "OPTIMIZER = 'sgd'  # choice in ['sgd', 'adam']\n",
    "BATCH_SIZE = 9\n",
    "CROP_SIZE = (256, 256)\n",
    "LEARNING_RATE = 1e-2\n",
    "MILESTONES = [200, 400]\n",
    "MOMENTUM = 0.95\n",
    "MULT = 1\n",
    "NUM_EPOCHS = 500\n",
    "NUM_CPU_WORKERS = 0\n",
    "PRINT_EVERY = 1\n",
    "RANDOM_SEED = 6\n",
    "WEIGHT_DECAY = 5e-4\n",
    "VAL_EVERY = 1\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# add a new entry here if creating a new data loader\n",
    "dataset_list = {\n",
    "    'mtc': MaizeTasselDataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    \"\"\"Parse all the arguments provided from the CLI.\n",
    "\n",
    "    Returns:\n",
    "      A list of parsed arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser(description=\"Object Counting Framework\")\n",
    "    # constant\n",
    "    parser.add_argument(\"--image-scale\", type=float, default=IMG_SCALE, help=\"Scale factor used in normalization.\")\n",
    "    parser.add_argument(\"--image-mean\", nargs='+', type=float, default=IMG_MEAN, help=\"Mean used in normalization.\")\n",
    "    parser.add_argument(\"--image-std\", nargs='+', type=float, default=IMG_STD, help=\"Std used in normalization.\")\n",
    "    parser.add_argument(\"--scales\", type=int, default=SCALES, help=\"Scales of crop.\")\n",
    "    parser.add_argument(\"--shorter-side\", type=int, default=SHORTER_SIDE, help=\"Shorter side of the image.\")\n",
    "    # system-related parameters\n",
    "    parser.add_argument(\"--data-dir\", type=str, default=DATA_DIR, help=\"Path to the directory containing the dataset.\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default=DATASET, help=\"Dataset type.\")\n",
    "    parser.add_argument(\"--exp\", type=str, default=EXP, help=\"Experiment path.\")\n",
    "    parser.add_argument(\"--data-list\", type=str, default=DATA_LIST,\n",
    "                        help=\"Path to the file listing the images in the dataset.\")\n",
    "    parser.add_argument(\"--data-val-list\", type=str, default=DATA_VAL_LIST,\n",
    "                        help=\"Path to the file listing the images in the val dataset.\")\n",
    "    parser.add_argument(\"--restore-from\", type=str, default=RESTORE_FROM, help=\"Name of restored model.\")\n",
    "    parser.add_argument(\"--snapshot-dir\", type=str, default=SNAPSHOT_DIR, help=\"Where to save snapshots of the model.\")\n",
    "    parser.add_argument(\"--result-dir\", type=str, default=RESULT_DIR, help=\"Where to save inferred results.\")\n",
    "    parser.add_argument(\"--save-output\", action=\"store_true\", help=\"Whether to save the output.\")\n",
    "    # model-related parameters\n",
    "    parser.add_argument(\"--input-size\", type=int, default=INPUT_SIZE, help=\"the minimum input size of the model.\")\n",
    "    parser.add_argument(\"--output-stride\", type=int, default=OUTPUT_STRIDE, help=\"Output stride of the model.\")\n",
    "    parser.add_argument(\"--resize-ratio\", type=float, default=RESIZE_RATIO, help=\"Resizing ratio.\")\n",
    "    parser.add_argument(\"--model\", type=str, default=MODEL, help=\"model to be chosen.\")\n",
    "    parser.add_argument(\"--use-pretrained\", action=\"store_true\", help=\"Whether to use pretrained model.\")\n",
    "    parser.add_argument(\"--freeze-bn\", action=\"store_true\", help=\"Whether to freeze encoder bnorm layers.\")\n",
    "    parser.add_argument(\"--sync-bn\", action=\"store_true\", help=\"Whether to apply synchronized batch normalization.\")\n",
    "    # training-related parameters\n",
    "    parser.add_argument(\"--optimizer\", type=str, default=OPTIMIZER, choices=['sgd', 'adam'], help=\"Choose optimizer.\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=BATCH_SIZE,\n",
    "                        help=\"Number of images sent to the network in one step.\")\n",
    "    parser.add_argument(\"--milestones\", nargs='+', type=int, default=MILESTONES, help=\"Multistep policy.\")\n",
    "    parser.add_argument(\"--crop-size\", nargs='+', type=int, default=CROP_SIZE, help=\"Size of crop.\")\n",
    "    parser.add_argument(\"--evaluate-only\", action=\"store_true\", help=\"Whether to perform evaluation.\")\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=LEARNING_RATE, help=\"Base learning rate for training.\")\n",
    "    parser.add_argument(\"--momentum\", type=float, default=MOMENTUM, help=\"Momentum component of the optimizer.\")\n",
    "    parser.add_argument(\"--weight-decay\", type=float, default=WEIGHT_DECAY,\n",
    "                        help=\"Regularisation parameter for L2-loss.\")\n",
    "    parser.add_argument(\"--mult\", type=float, default=MULT, help=\"LR multiplier for pretrained layers.\")\n",
    "    parser.add_argument(\"--num-epochs\", type=int, default=NUM_EPOCHS, help=\"Number of training epochs.\")\n",
    "    parser.add_argument(\"--num-workers\", type=int, default=NUM_CPU_WORKERS, help=\"Number of CPU cores used.\")\n",
    "    parser.add_argument(\"--print-every\", type=int, default=PRINT_EVERY, help=\"Print information every often.\")\n",
    "    parser.add_argument(\"--random-seed\", type=int, default=RANDOM_SEED,\n",
    "                        help=\"Random seed to have reproducible results.\")\n",
    "    parser.add_argument(\"--val-every\", type=int, default=VAL_EVERY, help=\"How often performing validation.\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def save_checkpoint(state, snapshot_dir, filename='model_ckpt.pth.tar'):\n",
    "    torch.save(state, '{}/{}'.format(snapshot_dir, filename))\n",
    "\n",
    "\n",
    "def plot_learning_curves(net, dir_to_save):\n",
    "    # plot learning curves\n",
    "    fig = plt.figure(figsize=(16, 9))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.plot(net.train_loss['epoch_loss'], label='train loss', color='tab:blue')\n",
    "    ax1.legend(loc='upper right')\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.plot(net.val_loss['epoch_loss'], label='val mae', color='tab:orange')\n",
    "    ax2.legend(loc='upper right')\n",
    "    # ax2.set_ylim((0,50))\n",
    "    fig.savefig(os.path.join(dir_to_save, 'learning_curves.png'), bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def train(net, train_loader, criterion, optimizer, epoch, args):\n",
    "    # switch to 'train' mode\n",
    "    net.train()\n",
    "\n",
    "    # uncomment the following line if the training images don't have the same size\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    if args.batch_size == 1:\n",
    "        for m in net.modules():\n",
    "            if isinstance(m, nn.BatchNorm2d):\n",
    "                m.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    avg_frame_rate = 0.0\n",
    "    in_sz = args.input_size\n",
    "    os = args.output_stride\n",
    "    target_filter = torch.cuda.FloatTensor(1, 1, in_sz, in_sz).fill_(1)\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        torch.cuda.synchronize()\n",
    "        start = time()\n",
    "\n",
    "        inputs, targets = sample['image'], sample['target']\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = net(inputs, is_normalize=False)\n",
    "        # generate targets\n",
    "        targets = F.conv2d(targets, target_filter, stride=os)\n",
    "        # compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # collect and print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        end = time()\n",
    "\n",
    "        running_frame_rate = args.batch_size * float(1 / (end - start))\n",
    "        avg_frame_rate = (avg_frame_rate * i + running_frame_rate) / (i + 1)\n",
    "        if i % args.print_every == args.print_every - 1:\n",
    "            print('epoch: %d, train: %d/%d, '\n",
    "                  'loss: %.5f, frame: %.2fHz/%.2fHz' % (\n",
    "                      epoch,\n",
    "                      i + 1,\n",
    "                      len(train_loader),\n",
    "                      running_loss / (i + 1),\n",
    "                      running_frame_rate,\n",
    "                      avg_frame_rate\n",
    "                  ))\n",
    "    net.train_loss['epoch_loss'].append(running_loss / (i + 1))\n",
    "\n",
    "\n",
    "def validate(net, valset, val_loader, criterion, epoch, args):\n",
    "    # switch to 'eval' mode\n",
    "    net.eval()\n",
    "    cudnn.benchmark = False\n",
    "\n",
    "    image_list = valset.image_list\n",
    "\n",
    "    if args.save_output:\n",
    "        epoch_result_dir = os.path.join(args.result_dir, str(epoch))\n",
    "        if not os.path.exists(epoch_result_dir):\n",
    "            os.makedirs(epoch_result_dir)\n",
    "        cmap = plt.cm.get_cmap('jet')\n",
    "\n",
    "    pd_counts = []\n",
    "    gt_counts = []\n",
    "    with torch.no_grad():\n",
    "        avg_frame_rate = 0.0\n",
    "        for i, sample in enumerate(val_loader):\n",
    "            torch.cuda.synchronize()\n",
    "            start = time()\n",
    "\n",
    "            image, gtcount = sample['image'], sample['gtcount']\n",
    "            # inference\n",
    "            output = net(image.cuda(), is_normalize=not args.save_output)\n",
    "            if args.save_output:\n",
    "                output_save = output\n",
    "                # normalization\n",
    "                output = Normalizer.gpu_normalizer(output, image.size()[2], image.size()[3], args.input_size,\n",
    "                                                   args.output_stride)\n",
    "            # postprocessing\n",
    "            output = np.clip(output, 0, None)\n",
    "\n",
    "            pdcount = output.sum()\n",
    "            gtcount = float(gtcount.numpy())\n",
    "\n",
    "            if args.save_output:\n",
    "                _, image_name = os.path.split(image_list[i])\n",
    "                output_save = np.clip(output_save.squeeze().cpu().numpy(), 0, None)\n",
    "                output_save = recover_countmap(output_save, image, args.input_size, args.output_stride)\n",
    "                output_save = output_save / (output_save.max() + 1e-12)\n",
    "                output_save = cmap(output_save) * 255.\n",
    "                # image composition\n",
    "                image = valset.images[image_list[i]]\n",
    "                nh, nw = output_save.shape[:2]\n",
    "                image = cv2.resize(image, (nw, nh), interpolation=cv2.INTER_CUBIC)\n",
    "                output_save = 0.5 * image + 0.5 * output_save[:, :, 0:3]\n",
    "\n",
    "                dotimage = valset.dotimages[image_list[i]]\n",
    "\n",
    "                fig = plt.figure()\n",
    "                ax1 = fig.add_subplot(1, 2, 1)\n",
    "                ax1.imshow(dotimage.astype(np.uint8))\n",
    "                ax1.get_xaxis().set_visible(False)\n",
    "                ax1.get_yaxis().set_visible(False)\n",
    "                ax2 = fig.add_subplot(1, 2, 2)\n",
    "                ax2.imshow(output_save.astype(np.uint8))\n",
    "                ax2.get_xaxis().set_visible(False)\n",
    "                ax2.get_yaxis().set_visible(False)\n",
    "                fig.suptitle('manual count=%4.2f, inferred count=%4.2f' % (gtcount, pdcount), fontsize=10)\n",
    "                if args.dataset == 'mtc':\n",
    "                    plt.tight_layout(rect=[0, 0, 1, 1.4])  # maize tassels counting\n",
    "                elif args.dataset == 'wec':\n",
    "                    plt.tight_layout(rect=[0, 0, 1, 1.45])  # wheat ears counting\n",
    "                elif args.dataset == 'shc':\n",
    "                    plt.tight_layout(rect=[0, 0, 0.95, 1])  # sorghum heads counting -- dataset1\n",
    "                    # plt.tight_layout(rect=[0, 0, 1.2, 1]) # sorghum heads counting -- dataset2\n",
    "                plt.savefig(os.path.join(epoch_result_dir, image_name.replace('.jpg', '.png')), bbox_inches='tight',\n",
    "                            dpi=300)\n",
    "                plt.close()\n",
    "\n",
    "            # compute mae and mse\n",
    "            pd_counts.append(pdcount)\n",
    "            gt_counts.append(gtcount)\n",
    "            mae = compute_mae(pd_counts, gt_counts)\n",
    "            mse = compute_mse(pd_counts, gt_counts)\n",
    "            rmae, rmse = compute_relerr(pd_counts, gt_counts)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            end = time()\n",
    "\n",
    "            running_frame_rate = 1 * float(1 / (end - start))\n",
    "            avg_frame_rate = (avg_frame_rate * i + running_frame_rate) / (i + 1)\n",
    "            if i % args.print_every == args.print_every - 1:\n",
    "                print(\n",
    "                    'epoch: {0}, test: {1}/{2}, pre: {3:.2f}, gt:{4:.2f}, me:{5:.2f}, mae: {6:.2f}, mse: {7:.2f}, rmae: {8:.2f}%, rmse: {9:.2f}%, frame: {10:.2f}Hz/{11:.2f}Hz'\n",
    "                        .format(epoch, i + 1, len(val_loader), pdcount, gtcount, pdcount - gtcount, mae, mse, rmae,\n",
    "                                rmse, running_frame_rate, avg_frame_rate)\n",
    "                )\n",
    "            start = time()\n",
    "    r2 = rsquared(pd_counts, gt_counts)\n",
    "    np.save(args.snapshot_dir + '/pd.npy', pd_counts)\n",
    "    np.save(args.snapshot_dir + '/gt.npy', gt_counts)\n",
    "    print('epoch: {0}, mae: {1:.2f}, mse: {2:.2f}, rmae: {3:.2f}%, rmse: {4:.2f}%, r2: {5:.4f}'.format(epoch, mae, mse,\n",
    "                                                                                                       rmae, rmse, r2))\n",
    "    # write to files        \n",
    "    with open(os.path.join(args.snapshot_dir, args.exp + '.txt'), 'a') as f:\n",
    "        print(\n",
    "            'epoch: {0}, mae: {1:.2f}, mse: {2:.2f}, rmae: {3:.2f}%, rmse: {4:.2f}%, r2: {5:.4f}'.format(epoch, mae,\n",
    "                                                                                                         mse, rmae,\n",
    "                                                                                                         rmse, r2),\n",
    "            file=f\n",
    "        )\n",
    "    with open(os.path.join(args.snapshot_dir, 'counts.txt'), 'a') as f:\n",
    "        for pd, gt in zip(pd_counts, gt_counts):\n",
    "            print(\n",
    "                '{0} {1}'.format(pd, gt),\n",
    "                file=f\n",
    "            )\n",
    "    # save stats\n",
    "    net.val_loss['epoch_loss'].append(mae)\n",
    "    net.measure['mae'].append(mae)\n",
    "    net.measure['mse'].append(mse)\n",
    "    net.measure['rmae'].append(rmae)\n",
    "    net.measure['rmse'].append(rmse)\n",
    "    net.measure['r2'].append(r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--image-scale IMAGE_SCALE] [--image-mean IMAGE_MEAN [IMAGE_MEAN ...]]\n",
      "                             [--image-std IMAGE_STD [IMAGE_STD ...]] [--scales SCALES] [--shorter-side SHORTER_SIDE]\n",
      "                             [--data-dir DATA_DIR] [--dataset DATASET] [--exp EXP] [--data-list DATA_LIST]\n",
      "                             [--data-val-list DATA_VAL_LIST] [--restore-from RESTORE_FROM]\n",
      "                             [--snapshot-dir SNAPSHOT_DIR] [--result-dir RESULT_DIR] [--save-output]\n",
      "                             [--input-size INPUT_SIZE] [--output-stride OUTPUT_STRIDE] [--resize-ratio RESIZE_RATIO]\n",
      "                             [--model MODEL] [--use-pretrained] [--freeze-bn] [--sync-bn] [--optimizer {sgd,adam}]\n",
      "                             [--batch-size BATCH_SIZE] [--milestones MILESTONES [MILESTONES ...]]\n",
      "                             [--crop-size CROP_SIZE [CROP_SIZE ...]] [--evaluate-only] [--learning-rate LEARNING_RATE]\n",
      "                             [--momentum MOMENTUM] [--weight-decay WEIGHT_DECAY] [--mult MULT]\n",
      "                             [--num-epochs NUM_EPOCHS] [--num-workers NUM_WORKERS] [--print-every PRINT_EVERY]\n",
      "                             [--random-seed RANDOM_SEED] [--val-every VAL_EVERY]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\User\\AppData\\Roaming\\jupyter\\runtime\\kernel-4eb333d0-83b5-4c3c-abde-9ab7d2f85d22.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3426: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "args = get_arguments()\n",
    "\n",
    "# args.evaluate_only = True\n",
    "# args.save_output = True\n",
    "\n",
    "args.image_mean = np.array(args.image_mean).reshape((1, 1, 3))\n",
    "args.image_std = np.array(args.image_std).reshape((1, 1, 3))\n",
    "\n",
    "args.crop_size = tuple(args.crop_size) if len(args.crop_size) > 1 else args.crop_size\n",
    "\n",
    "# seeding for reproducbility\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(args.random_seed)\n",
    "torch.manual_seed(args.random_seed)\n",
    "np.random.seed(args.random_seed)\n",
    "\n",
    "# instantiate dataset\n",
    "dataset = dataset_list[args.dataset]\n",
    "\n",
    "args.snapshot_dir = os.path.join(args.snapshot_dir, args.dataset.lower(), args.exp)\n",
    "if not os.path.exists(args.snapshot_dir):\n",
    "    os.makedirs(args.snapshot_dir)\n",
    "\n",
    "args.result_dir = os.path.join(args.result_dir, args.dataset.lower(), args.exp)\n",
    "if not os.path.exists(args.result_dir):\n",
    "    os.makedirs(args.result_dir)\n",
    "\n",
    "args.restore_from = os.path.join(args.snapshot_dir, args.restore_from)\n",
    "\n",
    "arguments = vars(args)\n",
    "for item in arguments:\n",
    "    print(item, ':\\t', arguments[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-dc3579237c44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_arguments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# args.evaluate_only = True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# args.save_output = True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-dcbce770a943>\u001b[0m in \u001b[0;36mget_arguments\u001b[1;34m()\u001b[0m\n\u001b[0;32m     50\u001b[0m                         help=\"Random seed to have reproducible results.\")\n\u001b[0;32m     51\u001b[0m     \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--val-every\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mVAL_EVERY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhelp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"How often performing validation.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[1;34m(self, args, namespace)\u001b[0m\n\u001b[0;32m   1769\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unrecognized arguments: %s'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1771\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1772\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1773\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\argparse.py\u001b[0m in \u001b[0;36merror\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m   2519\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2520\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'prog'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'message'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2521\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda3\\lib\\argparse.py\u001b[0m in \u001b[0;36mexit\u001b[1;34m(self, status, message)\u001b[0m\n\u001b[0;32m   2506\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2507\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2508\u001b[1;33m         \u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2510\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mSystemExit\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# instantiate network\n",
    "net = CountingModels(\n",
    "      arc=args.model,\n",
    "      input_size=args.input_size,\n",
    "      output_stride=args.output_stride\n",
    "    )\n",
    "\n",
    "net = nn.DataParallel(net)\n",
    "#net.cuda()\n",
    "\n",
    "# filter parameters\n",
    "learning_params = [p[1] for p in net.named_parameters()]\n",
    "pretrained_params = []\n",
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.L1Loss(reduction='mean').cuda()\n",
    "\n",
    "if args.optimizer == 'sgd':\n",
    "    optimizer = torch.optim.SGD(\n",
    "        [\n",
    "            {'params': learning_params},\n",
    "            {'params': pretrained_params, 'lr': args.learning_rate / args.mult},\n",
    "        ],\n",
    "        lr=args.learning_rate,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.weight_decay\n",
    "    )\n",
    "elif args.optimizer == 'adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [\n",
    "            {'params': learning_params},\n",
    "            {'params': pretrained_params, 'lr': args.learning_rate / args.mult},\n",
    "        ],\n",
    "        lr=args.learning_rate\n",
    "    )\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "# restore parameters\n",
    "start_epoch = 0\n",
    "net.train_loss = {\n",
    "    'running_loss': [],\n",
    "    'epoch_loss': []\n",
    "}\n",
    "net.val_loss = {\n",
    "    'running_loss': [],\n",
    "    'epoch_loss': []\n",
    "}\n",
    "net.measure = {\n",
    "    'mae': [],\n",
    "    'mse': [],\n",
    "    'rmae': [],\n",
    "    'rmse': [],\n",
    "    'r2': []\n",
    "}\n",
    "if args.restore_from is not None:\n",
    "    if os.path.isfile(args.restore_from):\n",
    "        checkpoint = torch.load(args.restore_from)\n",
    "        net.load_state_dict(checkpoint['state_dict'])\n",
    "        if 'epoch' in checkpoint:\n",
    "            start_epoch = checkpoint['epoch']\n",
    "        if 'optimizer' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        if 'train_loss' in checkpoint:\n",
    "            net.train_loss = checkpoint['train_loss']\n",
    "        if 'val_loss' in checkpoint:\n",
    "            net.val_loss = checkpoint['val_loss']\n",
    "        if 'measure' in checkpoint:\n",
    "            net.measure['mae'] = checkpoint['measure']['mae'] if 'mae' in checkpoint['measure'] else []\n",
    "            net.measure['mse'] = checkpoint['measure']['mse'] if 'mse' in checkpoint['measure'] else []\n",
    "            net.measure['rmae'] = checkpoint['measure']['rmae'] if 'rmae' in checkpoint['measure'] else []\n",
    "            net.measure['rmse'] = checkpoint['measure']['rmse'] if 'rmse' in checkpoint['measure'] else []\n",
    "            net.measure['r2'] = checkpoint['measure']['r2'] if 'r2' in checkpoint['measure'] else []\n",
    "        print(\"==> load checkpoint '{}' (epoch {})\"\n",
    "              .format(args.restore_from, start_epoch))\n",
    "    else:\n",
    "        with open(os.path.join(args.snapshot_dir, args.exp + '.txt'), 'a') as f:\n",
    "            for item in arguments:\n",
    "                print(item, ':\\t', arguments[item], file=f)\n",
    "        print(\"==> no checkpoint found at '{}'\".format(args.restore_from))\n",
    "\n",
    "# define transform\n",
    "transform_train = [\n",
    "    RandomCrop(args.crop_size),\n",
    "    RandomFlip(),\n",
    "    Normalize(\n",
    "        args.image_scale,\n",
    "        args.image_mean,\n",
    "        args.image_std\n",
    "    ),\n",
    "    ToTensor(),\n",
    "    ZeroPadding(args.output_stride)\n",
    "]\n",
    "transform_val = [\n",
    "    Normalize(\n",
    "        args.image_scale,\n",
    "        args.image_mean,\n",
    "        args.image_std\n",
    "    ),\n",
    "    ToTensor(),\n",
    "    ZeroPadding(args.output_stride)\n",
    "]\n",
    "composed_transform_train = transforms.Compose(transform_train)\n",
    "composed_transform_val = transforms.Compose(transform_val)\n",
    "\n",
    "# define dataset loader\n",
    "trainset = dataset(\n",
    "    data_dir=args.data_dir,\n",
    "    data_list=args.data_list,\n",
    "    ratio=args.resize_ratio,\n",
    "    train=True,\n",
    "    transform=composed_transform_train\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    trainset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "valset = dataset(\n",
    "    data_dir=args.data_dir,\n",
    "    data_list=args.data_val_list,\n",
    "    ratio=args.resize_ratio,\n",
    "    train=False,\n",
    "    transform=composed_transform_val\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    valset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print('alchemy start...')\n",
    "if args.evaluate_only:\n",
    "    validate(net, valset, val_loader, criterion, start_epoch, args)\n",
    "    return\n",
    "\n",
    "best_mae = 1000000.0\n",
    "resume_epoch = -1 if start_epoch == 0 else start_epoch\n",
    "scheduler = MultiStepLR(optimizer, milestones=args.milestones, gamma=0.1, last_epoch=resume_epoch)\n",
    "for epoch in range(start_epoch, args.num_epochs):\n",
    "    # train\n",
    "    train(net, train_loader, criterion, optimizer, epoch + 1, args)\n",
    "    if epoch % args.val_every == args.val_every - 1:\n",
    "        # val\n",
    "        validate(net, valset, val_loader, criterion, epoch + 1, args)\n",
    "        # save_checkpoint\n",
    "        state = {\n",
    "            'state_dict': net.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'epoch': epoch + 1,\n",
    "            'train_loss': net.train_loss,\n",
    "            'val_loss': net.val_loss,\n",
    "            'measure': net.measure\n",
    "        }\n",
    "        save_checkpoint(state, args.snapshot_dir, filename='model_ckpt.pth.tar')\n",
    "        if net.measure['mae'][-1] <= best_mae:\n",
    "            save_checkpoint(state, args.snapshot_dir, filename='model_best.pth.tar')\n",
    "            best_mae = net.measure['mae'][-1]\n",
    "            best_mse = net.measure['mse'][-1]\n",
    "            best_rmae = net.measure['rmae'][-1]\n",
    "            best_rmse = net.measure['rmse'][-1]\n",
    "            best_r2 = net.measure['r2'][-1]\n",
    "        print(args.exp + ' epoch {} finished!'.format(epoch + 1))\n",
    "        print('best mae: {0:.2f}, best mse: {1:.2f}, best_rmae: {2:.2f}, best_rmse: {3:.2f}, best_r2: {4:.4f}'\n",
    "              .format(best_mae, best_mse, best_rmae, best_rmse, best_r2))\n",
    "        plot_learning_curves(net, args.snapshot_dir)\n",
    "    scheduler.step()\n",
    "\n",
    "print('Experiments with ' + args.exp + ' done!')\n",
    "with open(os.path.join(args.snapshot_dir, args.exp + '.txt'), 'a') as f:\n",
    "    print(\n",
    "        'best mae: {0:.2f}, best mse: {1:.2f}, best_rmae: {2:.2f}, best_rmse: {3:.2f}, best_r2: {4:.4f}'\n",
    "            .format(best_mae, best_mse, best_rmae, best_rmse, best_r2),\n",
    "        file=f\n",
    "    )\n",
    "    print(\n",
    "        'overall best mae: {0:.2f}, overall best mse: {1:.2f}, overall best_rmae: {2:.2f}, overall best_rmse: {3:.2f}, overall best_r2: {4:.4f}'\n",
    "            .format(min(net.measure['mae']), min(net.measure['mse']), min(net.measure['rmae']),\n",
    "                    min(net.measure['rmse']), max(net.measure['r2'])),\n",
    "        file=f\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
